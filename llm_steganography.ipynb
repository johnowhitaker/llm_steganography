{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0524bc2b46df48cd9b968b3d38c911e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates:\n",
      " and with probability 0.26030680537223816\n",
      " not with probability 0.13933220505714417\n",
      " here with probability 0.12296022474765778\n",
      " trained with probability 0.058082301169633865\n",
      " please with probability 0.03991934284567833\n",
      "Choosing token  and\n",
      "Candidates:\n",
      " I with probability 0.6940591335296631\n",
      " today with probability 0.053520094603300095\n",
      " here with probability 0.044369716197252274\n",
      " you with probability 0.02864724025130272\n",
      " this with probability 0.02864724025130272\n",
      "Choosing token  today\n",
      "Candidates:\n",
      " I with probability 0.6163644790649414\n",
      " we with probability 0.2569389343261719\n",
      ", with probability 0.07361423969268799\n",
      "'s with probability 0.027081165462732315\n",
      " you with probability 0.008791966363787651\n",
      "Choosing token  I\n",
      "Candidates:\n",
      " will with probability 0.3834528923034668\n",
      "'ll with probability 0.2635430693626404\n",
      "'m with probability 0.14106443524360657\n",
      "'d with probability 0.05189470574259758\n",
      " would with probability 0.031475730240345\n",
      "Choosing token 'll\n",
      "Hello, I'm a language model, and today I'll\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Hello, I'm a language model,\", return_tensors=\"pt\")\n",
    "for i in range(4):\n",
    "    # Get output probabilities\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids).logits\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    probabilities = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "    # Look at the top 5 candidates\n",
    "    top_5 = torch.topk(probabilities, 5)\n",
    "    top_5_token_ids = top_5.indices\n",
    "    top_5_probabilities = top_5.values\n",
    "    print(\"Candidates:\")\n",
    "    for token, prob in zip(top_5_token_ids[0], top_5_probabilities[0]):\n",
    "        print(f\"{tokenizer.decode(token.item())} with probability {prob.item()}\")\n",
    "\n",
    "    # Option 1: Choose the most likely candidate\n",
    "    # next_token = top_5_token_ids[0, 0]\n",
    "    # print(f\"Choosing token {tokenizer.decode(next_token.item())}\")\n",
    "\n",
    "    # Option 2: Alternating between first and second candidate\n",
    "    next_token = top_5_token_ids[0, i % 2]\n",
    "    print(f\"Choosing token {tokenizer.decode(next_token.item())}\")\n",
    "\n",
    "    # Add the new token to the input\n",
    "    input_ids = torch.cat([input_ids, next_token[None, None]], dim=-1)\n",
    "\n",
    "print(tokenizer.decode(input_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text -> Binary\n",
    "\n",
    "There are many options, this 5-bit version hacked together for demonstration by my new buddy Sonnet 3.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello World\n",
      "Encoded: 11111010000010101100011000111100000111111011101111100100110000100\n",
      "Decoded: Hello World\n"
     ]
    }
   ],
   "source": [
    "# Define the character set for 5-bit encoding\n",
    "char_set = \" abcdefghijklmnopqrstuvwxyz.,!?\"\n",
    "char_to_bin = {c: f\"{i:05b}\" for i, c in enumerate(char_set)}\n",
    "bin_to_char = {v: k for k, v in char_to_bin.items()}\n",
    "\n",
    "def encode_secret_message(txt):\n",
    "    encoded = \"\"\n",
    "    for c in txt:\n",
    "        if c.lower() in char_set:\n",
    "            if c != c.lower():\n",
    "                encoded += \"11111\"\n",
    "            encoded += char_to_bin[c.lower()]\n",
    "      \n",
    "    return encoded\n",
    "\n",
    "def decode_secret_message(binary_string):\n",
    "    chunks = [binary_string[i:i+5] for i in range(0, len(binary_string), 5)]\n",
    "    decoded = \"\"\n",
    "    uppercase_next = False\n",
    "    for chunk in chunks:\n",
    "        if chunk == \"11111\":\n",
    "            uppercase_next = True\n",
    "        else:\n",
    "            char = bin_to_char.get(chunk, \"\")\n",
    "            if uppercase_next:\n",
    "                char = char.upper()\n",
    "                uppercase_next = False\n",
    "            decoded += char\n",
    "    return decoded\n",
    "\n",
    "t = \"Hello World\"\n",
    "e = encode_secret_message(t)\n",
    "d = decode_secret_message(e)\n",
    "print(f\"Original: {t}\")\n",
    "print(f\"Encoded: {e}\")\n",
    "print(f\"Decoded: {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Binary with the choice of token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796cf14f8bb548ecad498866339377fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steganographic text:\n",
      "('When I was young, we used to visit my grandmother in the countryside. She '\n",
      " 'always bakes the most delicious pies. She used a special pie dish that I '\n",
      " 'loved. It was a round, white ceramic pie plate. I loved the shape and the '\n",
      " 'color. I loved the way it looked when she took the pie out of the oven. I '\n",
      " 'loved how the pie crust looked on the plate.\\n'\n",
      " 'I have always wanted to have one. But I never did. I donâ€™t know if I was '\n",
      " \"afraid to ask for it, or I just didn't know how to get\")\n"
     ]
    }
   ],
   "source": [
    "def generate_steganographic_text(prompt, secret_message):\n",
    "    # Encode start prompt and trim to multiple of 5\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_ids = input_ids[:, :((input_ids.size(1) // 5) * 5)]\n",
    "    # Turn message into binary string and add tokens to input_ids\n",
    "    encoded = encode_secret_message(secret_message)\n",
    "    for bit in tqdm(encoded):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids).logits\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        probabilities = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        top_5_token_ids = torch.topk(probabilities, 5).indices\n",
    "        next_token = top_5_token_ids[0, 0] if bit == \"0\" else top_5_token_ids[0, 1]\n",
    "        input_ids = torch.cat([input_ids, next_token[None, None]], dim=-1)\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"When I was young, we used to visit my grandmother in the countryside. \" +\\\n",
    "         \"She always bakes the most delicious pies.\"\n",
    "secret_message = \"Hawk at midnight!\"\n",
    "stego_text = generate_steganographic_text(prompt, secret_message)\n",
    "print(\"Steganographic text:\")\n",
    "pprint(stego_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ddh iHawk at midnigxt!'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_steganographic_text(text, start=0):\n",
    "    ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    binary_string = \"\"\n",
    "    with torch.no_grad():\n",
    "            logits = model(ids).logits # << Note: just one forward pass to get all logits\n",
    "    for i in range(start, len(ids[0])):\n",
    "        next_token_logits = logits[:, i-1, :]\n",
    "        top_5_token_ids = torch.topk(next_token_logits, 5).indices\n",
    "        if ids[0][i] == top_5_token_ids[0, 0]:\n",
    "            binary_string += \"0\"\n",
    "        elif ids[0][i] == top_5_token_ids[0, 1]:\n",
    "            binary_string += \"1\"\n",
    "        else: \n",
    "            binary_string += \"0\" # Not part of top5 (happens in prompt) - add 0 to keep length right.\n",
    "    return decode_secret_message(binary_string)\n",
    "\n",
    "decode_steganographic_text(stego_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hawk at midnigxt!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_steganographic_text(stego_text, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d Thanks fer watching!'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"\"\"In this video about steganography, we are talking to \\\n",
    "a person that is using steganography. We are asking him about \\\n",
    "the process and the reasons for doing it. We are asking about \\\n",
    "his experience and what are the benefits of using steganography.\n",
    "In this video, Iâ€™m talking about the process and the reasons for \\\n",
    "doing it. We are talking to someone who is using stegnography and \\\n",
    "asking him about his experience. Iâ€™m asking him about the benefits \\\n",
    "and the process.\\nStegano, or steganographic encryption is the practice \\\n",
    "where you embed...\"\"\"\n",
    "decode_steganographic_text(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda11-7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
